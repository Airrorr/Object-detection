# -*- coding: utf-8 -*-
"""Object detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GrEQc3bqOZMnjBkk8dI0DCzbggyYD_cL
"""

from keras import datasets

(trainimg,trainlb),(testimg,testlb) = datasets.cifar10.load_data()

import numpy
import matplotlib.pyplot as plt
from keras import models
from keras import layers
from keras.utils import to_categorical
from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization

print(trainimg.shape)
print(trainlb.shape)
print(testimg.shape)
print(testlb.shape)

trainimg = trainimg/255 # Rescale pixel intensity to between 0 and 1
trainlb = to_categorical(trainlb)

model = models.Sequential() # Start of convolutional neural network

model.add(layers.Conv2D(filters=32,kernel_size = (3,3), # Add convolutional layer with 32 filters, a 3x3 window, 
input_shape=(32,32,3),activation='relu')) # ReLU activation function
model.add(BatchNormalization()) # Batch normalization 

model.add(layers.Conv2D(filters=32,kernel_size = (3,3), # same process using same filters.
    activation='relu'))
model.add(BatchNormalization()) 

model.add(layers.MaxPooling2D(pool_size=(2,2))) # Add max pooling layer with a 2x2 window
model.add(Dropout(0.4)) # dropout layer with a rate of 40%

###############################################################
model.add(layers.Conv2D(filters=64,kernel_size = (3,3), # Changed filters to 64 to extract more patterns 
    activation='relu')) # ReLU activation function
model.add(BatchNormalization()) # Batch normalization 

model.add(layers.Conv2D(filters=64,kernel_size = (3,3), # Changed filters to 64 to extract more patterns
    activation='relu'))
model.add(BatchNormalization())
model.add(Dropout(0.6)) # dropout layer with a rate of 60%
###############################################################
model.add(layers.Conv2D(filters=128,kernel_size = (3,3), # Changed filters to 128 to extract more patterns
activation='relu'))
model.add(BatchNormalization())

model.add(layers.Conv2D(filters=128,kernel_size = (3,3), # Changed filters to 128 to extract more patterns
    activation='relu'))
model.add(BatchNormalization())

model.add(layers.MaxPooling2D(pool_size=(2,2))) # Add max pooling layer with a 2x2 window
model.add(Dropout(0.7)) # dropout layer with a rate of 70%
###############################################################

model.add(layers.Flatten()) # Add layer to flatten input
model.add(layers.Dense(256, # Add fully connected layer of 256 units 
     activation='relu')) # with a ReLU activation function
model.add(BatchNormalization()) 
model.add(Dropout(0.7)) # dropout layer with a rate of 70%

model.add(layers.Dense(512, # Add fully connected layer of 512 units 
    activation='relu')) # with a ReLU activation function
model.add(BatchNormalization())
model.add(Dropout(0.8)) # dropout layer with a rate of 80%

model.add(layers.Dense(10,activation='softmax')) # Add fully connected layer with a softmax activation function
# Compile neural network 
model.compile(loss='categorical_crossentropy', # Cross-entropy
              metrics=['accuracy'], # Accuracy performance metric
              optimizer='adam') # Adam Optimizer 

# Train neural network
model.fit(trainimg,trainlb,
          epochs=80,# Number of epochs
             batch_size=128,# Number of observations per batch
                 validation_split=0.2) # evaluation





model.summary()



